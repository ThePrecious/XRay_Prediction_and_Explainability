{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgCVpNOeJei_"
   },
   "source": [
    "In this demo file, we investigate methods for explainability in medical imaging using pre-trained TorchXRayVision models ( https://github.com/mlmed/torchxrayvision )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-y0MPKAKMPZ"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "id": "k5j6PvInJdZn",
    "outputId": "9b34606b-cb4a-437b-f557-374c729a886b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'gifsplanation' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mlmed/gifsplanation\n",
    "\n",
    "import numpy as np\n",
    "import torchxrayvision as xrv\n",
    "import skimage, torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import sys,os\n",
    "sys.path.insert(0,\"./gifsplanation/\")\n",
    "#sys.path.insert(0,\"./torchxrayvision/torchxrayvision\")\n",
    "\n",
    "from captum.attr import IntegratedGradients, Saliency, InputXGradient\n",
    "import attribution\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gC1DVSQrJmG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98XN5xm4MbTB"
   },
   "source": [
    "# Investigating the models\n",
    "\n",
    "In the following code cell, we investigate the models included in the TorchXRayVision package. Our ultimate goal is to build an app to help us visualize and interpret model predictions based on the input XRay image. This necessarily requires us to expose the model and access its layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JafegeKMkED",
    "outputId": "2393174d-e40f-4f35-8ab4-dd39910bab14"
   },
   "outputs": [],
   "source": [
    "# first, we define a test image\n",
    "!wget https://raw.githubusercontent.com/mlmed/torchxrayvision/master/tests/16747_3_1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "a0jo8-eG4Rca",
    "outputId": "ed474af7-a9e5-4a7a-90c2-4e54d13360dc"
   },
   "outputs": [],
   "source": [
    "#view the test image\n",
    "img = skimage.io.imread(\"16747_3_1.jpg\")\n",
    "img = xrv.datasets.normalize(img, 255) # convert 8-bit image to [-1024, 1024] range\n",
    "img = img.mean(2)[None, ...] # Make single color channel by averaging across the channel dimension, also add an empty batch dimension up front\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "A6DhHS1RnJqk",
    "outputId": "e7722751-eb4c-4781-feb1-1f330062b01f"
   },
   "outputs": [],
   "source": [
    "# convert that test image to a torch.tensor compatible with our models, i.e., size = (224,224)\n",
    "transform = torchvision.transforms.Compose([xrv.datasets.XRayCenterCrop(),xrv.datasets.XRayResizer(224)])\n",
    "img = transform(img)\n",
    "img = torch.from_numpy(img)\n",
    "\n",
    "plt.imshow(img.numpy()[0], cmap = 'gray') #resized image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6AdCg5aPADT",
    "outputId": "75b266e1-1eb2-4712-bc4a-2083e9006c8c"
   },
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model_names = [\"densenet121-res224-all\", \"densenet121-res224-rsna\", \"densenet121-res224-nih\",\n",
    "               \"densenet121-res224-pc\", \"densenet121-res224-chex\", \"densenet121-res224-mimic_nb\", \n",
    "               \"densenet121-res224-mimic_ch\", \"resnet50-res512-all\"]\n",
    "model = xrv.models.DenseNet(weights=model_names[0]).to(device)\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nga1HLrPn8nh",
    "outputId": "d4b75a3d-66ef-4631-87d7-a14cf91f86ae"
   },
   "outputs": [],
   "source": [
    "preds = model(img[None,...])\n",
    "# the model predictions are in logits, i.e., before the soft max layer.  \n",
    "dict(zip(model.pathologies,preds[0].detach().numpy()))\n",
    "# This information should be presented as a horizontal bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "2kNOHiyIqmkP",
    "outputId": "85d73177-a373-48a0-81fc-5972ed52cd52"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.style.use('ggplot')\n",
    "plt.barh(model.pathologies,preds[0].detach().numpy())\n",
    "plt.title('Outputs')\n",
    "plt.ylabel('Pathology')\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvOs8Fy6MkpQ"
   },
   "source": [
    "# Gradient-based saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikVOEWwjhspD"
   },
   "outputs": [],
   "source": [
    "##\n",
    "## It turns out, we don't actually have to implement these methods by hand. \n",
    "## The Captum package has all the functionality we need\n",
    "##\n",
    "\n",
    "# def input_gradient(input, model):\n",
    "#     # Computes the gradient of the model output (logit) wrt the input image;\n",
    "#     # basically local sensitivity analysis of model predictions wrt input images\n",
    "#     # input is (num_batch=1,num_channels=1, height, width) to match the model input\n",
    "\n",
    "#     model.eval() #checking we are in the appropriate mode\n",
    "\n",
    "#     #no training, so no grads wrt params needed\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "#     input.requires_grad = True #size (1,1,224,224)\n",
    "\n",
    "#     preds = model(input)\n",
    "    \n",
    "#     output_of_interest = preds.max()\n",
    "#     ## TODO: In reality, the user may want to see the input gradient wrt other outputs / classes.\n",
    "#     # This function should also accept an index for which class we want the saliency map.\n",
    "#     # sorted, indices = preds.sort()\n",
    "#     # output_of_interest = sorted.squeeze()[-1]\n",
    "    \n",
    "#     output_of_interest.backward()\n",
    "#     #for sensitivities, we are not interested in the sign of the gradient, just its magnitude\n",
    "#     sens = torch.abs(input.grad) #size (1,1,224,224)\n",
    "#     sens = (sens - sens.min())/(sens.max()-sens.min()) #normalize local sensitivities for convenience\n",
    "#     return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYeNUX0SMxG7"
   },
   "outputs": [],
   "source": [
    "input = img[None,...]\n",
    "input.requires_grad_()\n",
    "\n",
    "saliency = Saliency(model)\n",
    "sens_attr = saliency.attribute(input, target=preds.argmax())\n",
    "# in our main implementation, the target should be user defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "sX8-8NXlgRYa",
    "outputId": "17056611-e962-4b77-f6c4-5b2f517e4f30"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img.numpy()[0], cmap = 'gray') #resized image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sens_attr[0,0].numpy(), cmap=plt.cm.hot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lm6DHjrl2gS"
   },
   "source": [
    "# Gradient X Input Saliency Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li5oVwg-l13B"
   },
   "outputs": [],
   "source": [
    "input = img[None,...]\n",
    "input.requires_grad_()\n",
    "\n",
    "input_x_gradient = InputXGradient(model)\n",
    "input_x_gradient_attr = input_x_gradient.attribute(input.to(device), target=preds.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "drThpMXvnWBr",
    "outputId": "357ae598-ea1a-4fb0-a939-a5800f5c1cf2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img.numpy()[0], cmap = 'gray') #resized image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.abs(input_x_gradient_attr[0,0].detach().numpy()), cmap=plt.cm.hot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfydRYH4NSHw"
   },
   "source": [
    "# Integrated gradient-based saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbgZ_h86ozkZ"
   },
   "outputs": [],
   "source": [
    "input = img[None,...]\n",
    "input.requires_grad_()\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "ig_attr = ig.attribute(input.to(device), target=preds.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "f-e5DqaeivEf",
    "outputId": "c3181dc5-0914-46f5-ccf2-3cf1612ad8ef"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img.numpy()[0], cmap = 'gray') #resized image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.abs(ig_attr[0,0].detach().numpy()), cmap=plt.cm.hot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrNvz_xjNW_U"
   },
   "source": [
    "# Gifsplanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "GUtb2K5eNbF8",
    "outputId": "15092d0d-2807-4e41-873a-4a5eb29400e5"
   },
   "outputs": [],
   "source": [
    "input = img[None,...]\n",
    "input.requires_grad=False\n",
    "\n",
    "ae = xrv.autoencoders.ResNetAE(weights=\"101-elastic\").to(device)\n",
    "target = \"Nodule\"\n",
    "params = attribution.compute_attribution(input.to(device), \"latentshift\", model, target, ret_params=True, ae=ae)\n",
    "\n",
    "dimgs = np.concatenate(params[\"dimgs\"],1)[0]\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,3), dpi=350)\n",
    "plt.imshow(np.concatenate(dimgs,1), interpolation='none', cmap=\"gray\");\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "-V_WKROv3osG",
    "outputId": "00d88451-ba27-4c65-d7cd-1c7e7ad1e207"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "attribution.generate_video(input, model, target, ae, target_filename=\"test\", border=False, show=True,\n",
    "                           ffmpeg_path=\"ffmpeg\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
